{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WALLABY Example Workflow Tutorial\n",
    "\n",
    "Here we provide an example of how Django can be used in a typical astronomy workflow. We will interact with the database using Python (enabled through the Django ORM, documentation for queries can be found [here](https://docs.djangoproject.com/en/3.1/topics/db/queries/)), use plots to compare the WALLABY detections and identify a subset of interest to explore further (possibly through other tools).\n",
    "\n",
    "In this notebook we will assume that users are familiar with the Django ORM query syntax. As such, we won't describe in detail what each of the queries are doing. An introductory notebook is provided for those looking to work through more examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- RUN THIS CELL FIRST -------\n",
    "\n",
    "# import Python standard libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import django\n",
    "from datetime import datetime\n",
    "\n",
    "# Django setup\n",
    "sys.path.append('src/')\n",
    "django.setup()\n",
    "\n",
    "# Import Django models\n",
    "from run.models import Run\n",
    "from instance.models import Instance\n",
    "from detection.models import Detection\n",
    "from products.models import Products\n",
    "\n",
    "from sources.models import Sources\n",
    "from comments.models import Comments\n",
    "from tag.models import Tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching for sources\n",
    "\n",
    "In this example let's analyse the detections in a certain patch of sky. These detections have been written into the WALLABY database so we will write a query to get all of that information into the appropriate data type in Python. First we'll import a bunch of libraries that we will need for this.\n",
    "\n",
    "**NOTE**: If you need libraries other than those defined below, you can install them using `! pip install <library>` from an empty cell. The `!` before `pip install` is required and indicates for the command to be run via the terminal rather than the Python interpreter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data analysis libraries\n",
    "\n",
    "import pandas as pd\n",
    "from astropy.io import fits\n",
    "import mmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation libraries\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from matplotlib import figure\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column names for tables\n",
    "\n",
    "Databases store objects in tables where rows represent entries and columns represent the data fields. We want to find the RA and Dec for the detections. These may be named differently so lets find the names for these fields in the detections table. We will list all of the parameters in the detections table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List column names and types for the detections table\n",
    "\n",
    "Detection._meta.fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great so we see that there are two columns named `ra` and `dec` in the table that are represented by floats (they are numerical). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter by parameter\n",
    "\n",
    "Now that we know the name of the parameter(s) of interest, we can construct queries based on these fields. Let's find all of the detections in the following region of sky:\n",
    "\n",
    "* 120 < RA < 140\n",
    "* -10 < Dec < 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the detections by RA and Dec\n",
    "\n",
    "detections_subset = Detection.objects.filter(ra__gte=120.0, ra__lte=140.0).filter(dec__gte=-10.0, dec__lte=10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many detections in our subset of interest?\n",
    "\n",
    "detections_subset.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print parameters for each of these detections\n",
    "\n",
    "For this subset of detections let's print the parameter values in a table. We'll store it as a data frame using [pandas](https://pandas.pydata.org/) for more convenient access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detection table\n",
    "\n",
    "detections_subset_df = pd.DataFrame(list(detections_subset.values()))\n",
    "print(detections_subset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "\n",
    "Let's create a plot for the peak flux of the source against the major axis. It might show some interesting pattern in the detections. We'll use `matplotlib` for these visualisations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot of detection subset (peak flux x major axis)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20, 10)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plt.scatter(detections_subset_df['ell_maj'], detections_subset_df['f_max'])\n",
    "\n",
    "for d in detections_subset:\n",
    "    point = (float(d.ell_maj), float(d.f_max))\n",
    "    ax.annotate(d.name, xy=point, textcoords='data')\n",
    "\n",
    "plt.axhline(0.02, color='red')\n",
    "plt.axvline(15, color='red')\n",
    "plt.ylabel(\"Peak Flux [Jy]\")\n",
    "plt.xlabel(\"Major Axis [pixels]\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A subset of the subset\n",
    "\n",
    "Suppose we find the sources with `f_max >= 0.02` and with `ell_maj >= 15` interesting. In the plot above this represents the top right quadrant of the red grid. We can perform a filter on our subset of data again to get the data points of interest. Let's not create a data frame for these since there probably won't be any need for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the unique id, name, peak flux and semi-major axis of the sources of interest.\n",
    "\n",
    "interesting_subset = detections_subset.filter(f_max__gte=0.02, ell_maj__gte=15)\n",
    "\n",
    "print(\"unique id | name | max flux | major axis \")\n",
    "for d in interesting_subset:\n",
    "    print(f\"{d.id} | {d.name} | {d.f_max} | {d.ell_maj}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have the names of the detections (and they match up with the names that we see in the plots) as well as the flux, major axis and unique id. With the unique id, we are able to find that row in the database every time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisations\n",
    "\n",
    "Now that we have identified detections of interest we can explore the products of the detection in the database. This includes information about the run, the SoFiA parameters used, and the output products from the observation. For example, if we wanted to visualise the moment 0 map and data cube, we could do that with the following section of code. Let's do that for the really bright detection: `WALLABY J085143-020813`.\n",
    "\n",
    "**NOTE**: The cube data we're looking to visualise is stored in [FITS](https://heasarc.gsfc.nasa.gov/docs/heasarc/fits.html) format. `astropy` has functions for reading from FITS files that we will use to open this data. Unfortunately, we're only able to read from disk so we will have to save the FITS bytes from the database query into a temporary file. Therefore, a temporary file will need to be created for each product you wish to investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View fields of the Products table\n",
    "\n",
    "Products._meta.fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moment 0 Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write FITS data to local file\n",
    "\n",
    "detection_id = 2600\n",
    "filename = 'tmp.fits'\n",
    "bright_detection = Products.objects.get(id=detection_id)\n",
    "moment_0_bytes = b''.join(list(bright_detection.moment0[:]))\n",
    "\n",
    "with open(filename, \"wb\") as f:\n",
    "    f.write(moment_0_bytes)\n",
    "\n",
    "hdul = fits.open(filename)\n",
    "image = hdul[0].data\n",
    "hdul.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise moment 0 map.\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 20)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cube\n",
    "\n",
    "The data cube is a 3D array. Rather than visualising the entire cube we will compare the 2D slices with a slider that allows the user to change the frequency channel. \n",
    "\n",
    "**NOTE**: Don't change the slider too quickly it will get super laggy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read cube into np array.\n",
    "\n",
    "detection_id = 2600\n",
    "filename = 'cube.fits'\n",
    "bright_detection = Products.objects.get(id=detection_id)\n",
    "cube_bytes = b''.join(list(bright_detection.cube[:]))\n",
    "\n",
    "with open(filename, \"wb\") as f:\n",
    "    f.write(cube_bytes)\n",
    "\n",
    "# write cube to np.array\n",
    "hdul = fits.open(filename)\n",
    "cube = hdul[0].data\n",
    "hdul.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive plot of cube with slider for channel.\n",
    "\n",
    "def cube_visualisation(c = 0):\n",
    "    plt.imshow(cube[c, :, :])\n",
    "    plt.show()\n",
    "\n",
    "interact(cube_visualisation, c=widgets.IntSlider(min=0, max=cube.shape[0], step=1, value=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this workflow we have\n",
    "\n",
    "* Queried the database for WALLABY detection data\n",
    "* Created a plot comparing properties of the detections\n",
    "* Identified interesting data points from the plot\n",
    "* Inspected products from the detections of interest.\n",
    "\n",
    "Hopefully it has been useful to run through this example workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
