{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "mineral-corpus",
   "metadata": {},
   "source": [
    "# WALLABY Catalgoue Management\n",
    "\n",
    "In this notebook we go through a WALLABY administrator workflow to demonstrate how these notebooks can be used for managing the content of the catalogue. This notebook will have write access to the WALLABY database, but will require an authorised user.\n",
    "\n",
    "We will go through two main workflows: the development of duplicate detection algorithms that can be included in `SoFiAX` or to be included as part of future WALLABY workflows in RADEC, and performing manual inspection of detections.\n",
    "\n",
    "\n",
    "### A. Development of SoFiAX duplicate algorithms\n",
    "\n",
    "These notebooks can provide a useful workspace to iterate and develop new algorithms for identifying duplicate detections. These can then be added to SoFiAX (or to a source finding pipeline) when complete. The duplicates identified by this new algorithm can then be investigated manually without writing to the database, and can be managed completely in this notebook.\n",
    "\n",
    "We will do the following to demonstrate this:\n",
    "\n",
    "1. Develop a simple algorithm for identifying duplicates\n",
    "2. Verify the algorithm creates the tags we expect.\n",
    "3. Retrieve objects based on tags.\n",
    "\n",
    "### B. Manual inspection for components\n",
    "\n",
    "Manual inspection and management of components can be done as a final step (after automated processes that are implemented in SoFiAX) in this notebook. A WALLABY administrator who identifies two or more detections that are actually a single source can tag, merge and re-run SoFiA and SoFiAX (assuming the cube size is small) in this notebook.\n",
    "\n",
    "1. Perform some visual inspection and tag detections\n",
    "2. Re-run SoFiA after identifying components of the same galaxy\n",
    "3. Re-run SoFiAX to generate outputs for this new source\n",
    "4. Delete components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-maple",
   "metadata": {},
   "source": [
    "# Import libraries + setup Django\n",
    "\n",
    "Starting point of every notebook that accesses Django models. Run the three cells below to import libraries/modules and setup Django."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-father",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import django\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-stewart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Django shell\n",
    "\n",
    "sys.path.append('SoFiAX_services/api/')\n",
    "os.environ[\"DJANGO_ALLOW_ASYNC_UNSAFE\"]=\"True\"\n",
    "os.environ[\"DJANGO_SETTINGS_MODULE\"] = \"api.settings\"\n",
    "os.environ[\"DJANGO_DATABASE_NAME\"]=\"sofiadb\"\n",
    "os.environ[\"DJANGO_DATABASE_USER\"]=\"admin\"\n",
    "os.environ[\"DJANGO_DATABASE_PASSWORD\"]=\"admin\"\n",
    "os.environ[\"DJANGO_DATABASE_HOST\"]=\"146.118.69.90\"\n",
    "django.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worse-truth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Django models\n",
    "\n",
    "from run.models import Run\n",
    "from instance.models import Instance\n",
    "from detection.models import Detection\n",
    "from products.models import Products\n",
    "\n",
    "from sources.models import Sources\n",
    "from comments.models import Comments\n",
    "from tag.models import Tag, TagDetection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-shoulder",
   "metadata": {},
   "source": [
    "# Develop SoFiAX duplicate detection algorithm\n",
    "\n",
    "We're going to go through the process of developing a very simple duplicate identification algorithm for detections in the database. This naive approach will just look at the position, flux (sum) and position angle similarity between any pair of detections. This algorithm is applied to all pairs of detections in our database, and if the differences are below some threshold for each property, we will visually inspect them and tag if appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-handbook",
   "metadata": {},
   "source": [
    "Let's implement this naive duplicate identification algorithm and then apply it on all pairs of detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-belfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function that we can test on pairs\n",
    "\n",
    "def identify_duplicate(d1, d2):\n",
    "    \"\"\"Simple and naive duplicate detection algorithm. Can\n",
    "    update threshold values as necessary.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Threshold values\n",
    "    pos_threshold = 10\n",
    "    flux_threshold = 5\n",
    "    kin_pa_threshold = 5\n",
    "    \n",
    "    # Compute differences between detections\n",
    "    pos_diff = np.linalg.norm(\n",
    "        np.array([d1.x, d1.y, d1.z]) - np.array([d2.x, d2.y, d2.z])\n",
    "    )\n",
    "    flux_diff = np.abs(d1.f_sum - d2.f_sum)\n",
    "    kin_pa_diff = np.abs(d1.kin_pa - d2.kin_pa)\n",
    "    \n",
    "    # Duplicate logic\n",
    "    if (pos_diff <= pos_threshold) & (flux_diff <= flux_threshold) & (kin_pa_diff <= kin_pa_threshold):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-virginia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function on each pair of detections. Let's also filter\n",
    "# out the detections that have the same name.\n",
    "\n",
    "detections = Detection.objects.all()\n",
    "N_detections = len(detections)\n",
    "\n",
    "detection_pair_ids = []\n",
    "\n",
    "for i in range(N_detections):\n",
    "    for j in range(i + 1, N_detections):\n",
    "        d1 = detections[i]\n",
    "        d2 = detections[j]\n",
    "        if identify_duplicate(d1, d2):\n",
    "            pair = (d1.id, d2.id)\n",
    "            detection_pair_ids.append(pair)\n",
    "            print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-tours",
   "metadata": {},
   "source": [
    "We have returned the `id`s for the detections that meet the duplicate identification function criteria and do not have the same name. We could automatically tag these (in the body of the function or as we apply them to pairs of detections). But we could also just inspect them now in the notebook.\n",
    "\n",
    "**NOTE**: the filter for name should be included in the original function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-exercise",
   "metadata": {},
   "source": [
    "## Visualisation \n",
    "\n",
    "Tobias has provided a new plotting script that we will use to compare these pairs of sources. Users should feel free to change the content of the cells below and install other libraries they would like to use for their own visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-nothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "\n",
    "import io\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "import astropy.units as u\n",
    "from astropy.io import fits\n",
    "from astropy.wcs import WCS\n",
    "from astropy.visualization import PercentileInterval\n",
    "from astroquery.skyview import SkyView\n",
    "from astropy.utils.data import clear_download_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-closing",
   "metadata": {},
   "outputs": [],
   "source": [
    "Products._meta.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-meditation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for generating plots for inspecting detections\n",
    "\n",
    "def retrieve_dss_image(longitude, latitude, width, height):\n",
    "    hdulist = SkyView.get_images(position=\"{}, {}\".format(longitude, latitude), survey=[\"DSS\"], coordinates=\"J2000\", projection=\"Tan\", pixels=\"{}, {}\".format(str(int(2400 * width)), str(int(2400 * height))), width=width*u.deg, height=height*u.deg);\n",
    "    return hdulist[0][0]\n",
    "\n",
    "def inspect_plot(detection):\n",
    "    # Plot figure size    \n",
    "    interval = PercentileInterval(95.0)\n",
    "    plt.rcParams[\"figure.figsize\"] = (12,12)\n",
    "    \n",
    "    # Retrieve products from database\n",
    "    products = Products.objects.get(detection=detection)\n",
    "    \n",
    "    # Open moment 0 image\n",
    "    with io.BytesIO() as buf:\n",
    "        buf.write(products.moment0)\n",
    "        buf.seek(0)\n",
    "        hdu_mom0 = fits.open(buf)[0]\n",
    "        wcs = WCS(hdu_mom0.header)\n",
    "        mom0 = hdu_mom0.data\n",
    "\n",
    "    # Open moment 1 image\n",
    "    with io.BytesIO() as buf:\n",
    "        buf.write(products.moment1)\n",
    "        buf.seek(0)\n",
    "        hdu_mom1 = fits.open(buf)[0]\n",
    "        mom1 = hdu_mom1.data\n",
    "\n",
    "    with io.BytesIO() as buf:\n",
    "        buf.write(b''.join(products.spectrum))\n",
    "        buf.seek(0)\n",
    "        spectrum = np.loadtxt(buf, dtype=\"float\", comments=\"#\", unpack=True)\n",
    "\n",
    "    # Extract coordinate information\n",
    "    nx = hdu_mom0.header[\"NAXIS1\"]\n",
    "    ny = hdu_mom0.header[\"NAXIS2\"]\n",
    "    clon, clat = wcs.all_pix2world(nx/2, ny/2, 0)\n",
    "    tmp1, tmp3 = wcs.all_pix2world(0, ny/2, 0)\n",
    "    tmp2, tmp4 = wcs.all_pix2world(nx, ny/2, 0)\n",
    "    width = np.rad2deg(math.acos(math.sin(np.deg2rad(tmp3)) * math.sin(np.deg2rad(tmp4)) + math.cos(np.deg2rad(tmp3)) * math.cos(np.deg2rad(tmp4)) * math.cos(np.deg2rad(tmp1 - tmp2))))\n",
    "    tmp1, tmp3 = wcs.all_pix2world(nx/2, 0, 0)\n",
    "    tmp2, tmp4 = wcs.all_pix2world(nx/2, ny, 0)\n",
    "    height = np.rad2deg(math.acos(math.sin(np.deg2rad(tmp3)) * math.sin(np.deg2rad(tmp4)) + math.cos(np.deg2rad(tmp3)) * math.cos(np.deg2rad(tmp4)) * math.cos(np.deg2rad(tmp1 - tmp2))))\n",
    "    \n",
    "    # Download DSS image from SkyView\n",
    "    hdu_opt = retrieve_dss_image(clon, clat, width, height)\n",
    "    wcs_opt = WCS(hdu_opt.header)\n",
    "    \n",
    "    # Plot moment 0\n",
    "    ax2 = plt.subplot(2, 2, 1, projection=wcs);\n",
    "    ax2.imshow(mom0, origin=\"lower\");\n",
    "    ax2.grid(color=\"grey\", ls=\"solid\");\n",
    "    ax2.set_xlabel(\"Right ascension (J2000)\");\n",
    "    ax2.set_ylabel(\"Declination (J2000)\");\n",
    "    ax2.tick_params(axis=\"x\", which=\"both\", left=False, right=False);\n",
    "    ax2.tick_params(axis=\"y\", which=\"both\", top=False, bottom=False);\n",
    "    ax2.set_title(\"moment 0\");\n",
    "\n",
    "    # Plot DSS image with HI contours\n",
    "    bmin, bmax = interval.get_limits(hdu_opt.data);\n",
    "    ax = plt.subplot(2, 2, 2, projection=wcs_opt);\n",
    "    ax.imshow(hdu_opt.data, origin=\"lower\");\n",
    "    ax.contour(hdu_mom0.data, transform=ax.get_transform(wcs), levels=np.logspace(2.0, 5.0, 10), colors=\"lightgrey\", alpha=1.0);\n",
    "    ax.grid(color=\"grey\", ls=\"solid\");\n",
    "    ax.set_xlabel(\"Right ascension (J2000)\");\n",
    "    ax.set_ylabel(\"Declination (J2000)\");\n",
    "    ax.tick_params(axis=\"x\", which=\"both\", left=False, right=False);\n",
    "    ax.tick_params(axis=\"y\", which=\"both\", top=False, bottom=False);\n",
    "    ax.set_title(\"DSS + moment 0\");\n",
    "\n",
    "    # Plot moment 1\n",
    "    bmin, bmax = interval.get_limits(mom1);\n",
    "    ax3 = plt.subplot(2, 2, 3, projection=wcs);\n",
    "    ax3.imshow(hdu_mom1.data, origin=\"lower\", vmin=bmin, vmax=bmax, cmap=plt.get_cmap(\"gist_rainbow\"));\n",
    "    ax3.grid(color=\"grey\", ls=\"solid\");\n",
    "    ax3.set_xlabel(\"Right ascension (J2000)\");\n",
    "    ax3.set_ylabel(\"Declination (J2000)\");\n",
    "    ax3.tick_params(axis=\"x\", which=\"both\", left=False, right=False);\n",
    "    ax3.tick_params(axis=\"y\", which=\"both\", top=False, bottom=False);\n",
    "    ax3.set_title(\"moment 1\");\n",
    "\n",
    "    # Plot spectrum\n",
    "    xaxis = spectrum[1] / 1e+6;\n",
    "    data  = 1000.0 * np.nan_to_num(spectrum[2]);\n",
    "    xmin = np.nanmin(xaxis);\n",
    "    xmax = np.nanmax(xaxis);\n",
    "    ymin = np.nanmin(data);\n",
    "    ymax = np.nanmax(data);\n",
    "    ymin -= 0.1 * (ymax - ymin);\n",
    "    ymax += 0.1 * (ymax - ymin);\n",
    "    ax4 = plt.subplot(2, 2, 4);\n",
    "    ax4.step(xaxis, data, where=\"mid\", color=\"royalblue\");\n",
    "    ax4.set_xlabel(\"Frequency (MHz)\");\n",
    "    ax4.set_ylabel(\"Flux density (mJy)\");\n",
    "    ax4.set_title(\"spectrum\");\n",
    "    ax4.grid(True);\n",
    "    ax4.set_xlim([xmin, xmax]);\n",
    "    ax4.set_ylim([ymin, ymax]);\n",
    "\n",
    "    plt.suptitle(detection.name.replace(\"_\", \" \").replace(\"-\", \"−\"), fontsize=16);\n",
    "    plt.show();\n",
    "\n",
    "    # Clean up\n",
    "    clear_download_cache(pkgname=\"astroquery\");\n",
    "    clear_download_cache();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-burden",
   "metadata": {},
   "source": [
    "Now that we have the ability to generate plots for comparing, let's do a comparison of a detection with a known mock detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-delicious",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect plots for detection objects.\n",
    "\n",
    "detection1 = Detection.objects.get(id=3603)\n",
    "detection2 = Detection.objects.get(id=4261)\n",
    "\n",
    "inspect_plot(detection1)\n",
    "inspect_plot(detection2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-native",
   "metadata": {},
   "source": [
    "These are duplicates and we know that because they have the same name in the database. Let's leave a tag on one of the detections so that we can refer back to it later (once we figure out what to do with it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stylish-tractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new tag for this duplicate\n",
    "\n",
    "tag_name = \"Manual inspect duplicate J164455-575029\"\n",
    "if Tag.objects.filter(tag_name=tag_name).exists():\n",
    "    tag = tag = Tag.objects.get(tag_name=tag_name)\n",
    "else:\n",
    "    tag = Tag.objects.get(tag_name=tag_name, added_at=datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-understanding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign tag to these detections\n",
    "\n",
    "TagDetection.objects.create(tag=tag, detection=detection1)\n",
    "TagDetection.objects.create(tag=tag, detection=detection2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-teacher",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_detections = [d.detection for d in TagDetection.objects.all()]\n",
    "print(tagged_detections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-elephant",
   "metadata": {},
   "source": [
    "Great! We see the tags that we have added for this pair of detections. If you see more it's becuase we've run the same cell multiple times (no harm done we can delete `TagDetection` objects easily. We have developed an algorithm that can identify and tag detections in the database. This can be included as part of SoFiAX or as part of a WALLABY source finding workflow in RADEC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-credits",
   "metadata": {},
   "source": [
    "# Manual inspection for components\n",
    "\n",
    "Suppose in the manual exploration of our detections we find that both detections are actually components of the same source. For the purposes of this tutorial, we'll just use the detections identified in the section above even though they don't appear to be components (`id=3603` and `id=4261`). In this case we would want to resolve this issue by re-running `sofia` on the joined cube.\n",
    "\n",
    "It is possible to run `/bin/bash` commands in the notebook by starting the command in a code cell with `!`. Or, you can use `os.system()` and pass the `bash` command as the argument. Let's check that `sofia` and `sofiax` are both installed by running them without arguments in the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-dubai",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sofia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-variance",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sofiax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legal-spanish",
   "metadata": {},
   "source": [
    "Neat so they are both installed. The steps for making use of this are:\n",
    "\n",
    "1. Save merged data cubes and masks as files locally\n",
    "2. Upload `sofia` parameter file(s)\n",
    "3. Run `sofia`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-pension",
   "metadata": {},
   "source": [
    "### 1. Save merged cube and mask locally.\n",
    "\n",
    "Retrieve the cubes and masks from the database and merge them into new cube and mask files. We will run `sofia` on these products. Script for merging detections taken from Tobias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-elephant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store cubes as files\n",
    "# We'll write a function for this and do it for the two detections we are interested in.\n",
    "# Note that we rely on libraries installed further up\n",
    "\n",
    "def merge_products(*argc):\n",
    "    \"\"\"Merge the cube and mask products for all provided detection objects.\n",
    "    Developed following `wallmerge.py` provided by Tobias.\n",
    "    \n",
    "    \"\"\"\n",
    "    detections = list(argc)\n",
    "    hdu_cubes = []\n",
    "    hdu_masks = []\n",
    "    cube_bufs = []\n",
    "    mask_bufs = []\n",
    "    \n",
    "    # read from Products database\n",
    "    for d in detections:\n",
    "        products = Products.objects.get(detection=d)\n",
    "        \n",
    "        # read cubes\n",
    "        cube_buf = io.BytesIO()\n",
    "        cube_buf.write(products.cube)\n",
    "        cube_buf.seek(0)\n",
    "        cube_bufs.append(cube_buf)\n",
    "        cube = fits.open(cube_buf)\n",
    "        hdu_cubes.append(cube)\n",
    "        \n",
    "        # read masks\n",
    "        mask_buf = io.BytesIO()\n",
    "        mask_buf.write(products.mask)\n",
    "        mask_buf.seek(0)\n",
    "        mask_bufs.append(mask_buf)\n",
    "        mask = fits.open(mask_buf)\n",
    "        hdu_masks.append(mask)\n",
    "            \n",
    "    hdu0_cubes = [hdu[0] for hdu in hdu_cubes]\n",
    "    hdu0_masks = [hdu[0] for hdu in hdu_masks]\n",
    "    \n",
    "    # Extract dimensions from all cubes\n",
    "    axes = int(hdu0_cubes[0].header[\"NAXIS\"]);\n",
    "    naxis = [[int(hdu0_cubes[cube].header[\"NAXIS{:d}\".format(axis + 1)]) for axis in range(axes)] for cube in range(len(hdu0_cubes))];\n",
    "    naxis_out = [0 for axis in range(axes)];\n",
    "    offset_out = [0 for axis in range(axes)];\n",
    "\n",
    "    # Extract reference pixels from all cubes\n",
    "    crpix = [[hdu0_cubes[cube].header[\"CRPIX{:d}\".format(axis + 1)] for axis in range(axes)] for cube in range(len(hdu0_cubes))];\n",
    "\n",
    "    # Determine output dimensions (adapted from Miriad task 'imcomb')\n",
    "    for axis in range(axes):\n",
    "            minpix = int(-crpix[0][axis]);\n",
    "            maxpix = int(-crpix[0][axis]) + naxis[0][axis];\n",
    "\n",
    "            for cube in range(1, len(hdu0_cubes)):\n",
    "                    minpix = min(minpix, int(-crpix[cube][axis]));\n",
    "                    maxpix = max(maxpix, int(-crpix[cube][axis]) + naxis[cube][axis]);\n",
    "\n",
    "            naxis_out[axis] = maxpix - minpix;\n",
    "            offset_out[axis] = -minpix;\n",
    "\n",
    "            for cube in range(len(hdu0_cubes)): crpix[cube][axis] -= offset_out[axis];\n",
    "\n",
    "    print(\"Output cube size: \" + str(naxis_out));\n",
    "\n",
    "    # Create empty output cube and mask\n",
    "    size_out = [naxis_out[axis] for axis in range(axes)];\n",
    "    cube_out = np.full(list(reversed(size_out)), np.nan, dtype=np.float32);\n",
    "    mask_out = np.full(list(reversed(size_out)), 0, dtype=np.int32);\n",
    "    hdu_data_out = fits.PrimaryHDU(data=cube_out, header=hdu0_cubes[0].header);\n",
    "    hdu_mask_out = fits.PrimaryHDU(data=mask_out, header=hdu0_masks[0].header);\n",
    "\n",
    "    # Update reference pixel\n",
    "    for axis in range(axes):\n",
    "            hdu_data_out.header.set(\"crpix{:d}\".format(axis + 1), offset_out[axis]);\n",
    "            hdu_mask_out.header.set(\"crpix{:d}\".format(axis + 1), offset_out[axis]);\n",
    "\n",
    "    # Copy individual cubelets into output cube\n",
    "    for cube in range(len(hdu0_cubes)):\n",
    "            pix_min = [int(-crpix[cube][axis]) for axis in range(axes)];\n",
    "            pix_max = [pix_min[axis] + naxis[cube][axis] for axis in range(axes)];\n",
    "            print(\"- Input cube \" + str(cube) + \" position: \" + str(pix_min) + \" - \" + str(pix_max));\n",
    "\n",
    "            slc = tuple([slice(pix_min[axes - axis - 1], pix_max[axes - axis - 1], 1) for axis in range(axes)]);\n",
    "            cube_out[slc] = hdu0_cubes[cube].data;\n",
    "\n",
    "    # Copy individual masklets into output mask\n",
    "    for mask in range(len(hdu0_masks)):\n",
    "            pix_min = [int(-crpix[mask][axis]) for axis in range(axes)];\n",
    "            pix_max = [pix_min[axis] + naxis[mask][axis] for axis in range(axes)];\n",
    "\n",
    "            slc = tuple([slice(pix_min[axes - axis - 1], pix_max[axes - axis - 1], 1) for axis in range(axes)]);\n",
    "            mask_out[slc] = hdu0_masks[mask].data;\n",
    "\n",
    "    # Close input files again\n",
    "    for f in cube_bufs:\n",
    "        f.close()\n",
    "        \n",
    "    for f in mask_bufs:\n",
    "        f.close()\n",
    "\n",
    "    # Write output cube and mask\n",
    "    hdu_data_out.writeto(\"wallmerge_output_cube.fits\", overwrite=True);\n",
    "    hdu_mask_out.writeto(\"wallmerge_output_mask.fits\", overwrite=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-macintosh",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_products(detection1, detection2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-property",
   "metadata": {},
   "source": [
    "### 2. Upload files\n",
    "\n",
    "To upload files you can click on the Jupyter icon in the top left of the notebook to go to a view of the environment. You should see this notebook as `investigate.ipynb` as well as three subdirectories `SoFiA-2`, `SoFiAX` and `SoFiAX_services`. These were required as part of installation of `sofia` and `sofiax`, and are used to ensure this notebook is able to connect with the database.\n",
    "\n",
    "We have a parameter file `sofia.par` that you may use and update through Jupyterhub with some useful default values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blessed-forward",
   "metadata": {},
   "source": [
    "### 3. Re-run `sofia`\n",
    "\n",
    "Since we are writing all of our products to a folder called `output` we have to make that first. Once that is done, we can run `sofia` with the parameter file provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-moment",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-denver",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sofia sofia.par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-native",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-panel",
   "metadata": {},
   "source": [
    "Great! So we're able to run `sofia` on merged detections. Life is good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-syndicate",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
